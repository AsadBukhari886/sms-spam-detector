                                                                                      AI & SMS Spam Detector

A real-time web application to analyze text messages for spam and detect AI-generated content using a machine learning model and the Groq API.
<img width="503" height="377" alt="image" src="https://github.com/user-attachments/assets/8c53102f-d75c-43e8-b54a-ed1b6a4872e5" />

<img width="496" height="408" alt="image" src="https://github.com/user-attachments/assets/4404cff0-3ae3-49e5-8149-f43e481dc70f" />


📋 Table of Contents
About The Project
Features
Tech Stack
Getting Started
Prerequisites
Installation
Project Structure
API Endpoint
How It Works

 About The Project
This project combines two powerful text analysis tools into one simple interface:
SMS Spam Detection: Utilizes a pre-trained scikit-learn model to classify incoming text messages as either "Spam" or "Not Spam" (Ham).
AI Content Detection: Leverages the speed of the Groq API with the Llama 3 model to estimate the probability that a given text was generated by an AI.
The application features a Python FastAPI backend for the logic and a React frontend for a responsive user experience.

Features
Dual Analysis: Get results for both spam and AI content from a single request.
Real-Time Results: Fast analysis powered by a lightweight model and the high-speed Groq API.
Clean UI: Simple and intuitive interface built with React.
RESTful API: A clear and easy-to-use API endpoint for text analysis.
Decoupled Architecture: Separate frontend and backend for better maintainability and scalability.

🛠️ Tech Stack
This project is built with a modern technology stack:
Backend:
Python 3.9+
FastAPI: For building the high-performance API.
Uvicorn: As the ASGI server.
Scikit-learn: For the spam classification model.
Joblib: For loading the trained model.
python-dotenv: For managing environment variables.
Frontend:
React.js
Axios: For making API requests to the backend.
CSS3: For styling.
Services:
Groq API: For fast AI-powered content detection.

 Getting Started
Follow these instructions to get a local copy of the project up and running.
Prerequisites
Make sure you have the following installed on your system:
Python (version 3.8 or higher)
Node.js (version 16 or higher) and npm
Git Installation

# Clone the Repository
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
# Backend Setup
# Navigate to the backend directory
cd backend
# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`

# Install the required Python packages
pip install -r requirements.txt

# Create a .env file for your API key
cp .env.example .env
Now, open the newly created .env file and add your Groq API key:

GROQ_API_KEY=gsk_YourSecretGroqApiKeyHere
Note: You will need to create a requirements.txt file if you don't have one. You can generate it by running pip freeze > requirements.txt after installing the packages (fastapi, uvicorn, scikit-learn, joblib, python-dotenv, requests).

# Frontend Setup
# Navigate to the frontend directory from the root
cd frontend

# Install the required npm packages
npm install
Running the Application
Start the Backend Server: In your terminal (from the backend directory):
uvicorn main:app --reload
The backend will be running at http://127.0.0.1:8000.
Start the Frontend Development Server: In a new terminal (from the frontend directory):
npm start
The React app will open automatically in your browser at http://localhost:3000.

📁 Project Structure

├── backend/
│   ├── .env                # Stores API keys (created by you)
│   ├── .venv        # Example environment file
│   ├── main.py             # FastAPI application logic
│   ├── spam_classifier.pkl # Pre-trained spam detection model
│   └── requirements.txt    # Python dependencies
│
└── frontend/
    ├── public/
    │   └── index.html
    ├── src/
    │   ├── App.css
    │   ├── App.js          # Main React component
    │   └── index.js
    ├── package.json
    └── ...
    
API Endpoint
The application exposes one primary API endpoint for analysis.
POST /analyze
Analyzes a given text for spam and AI content.
Request Body: code
JSON
{
  "text": "Your message text goes here."
}
Success Response (200 OK):

JSON
{
  "spam_detection": {
    "result": "Not Spam",
    "prediction_code": 0
  },
  "ai_detection": {
    "percentage": 15
  }
}
Error Response (AI Detection Failure):

JSON
{
  "spam_detection": {
    "result": "Not Spam",
    "prediction_code": 0
  },
  "ai_detection": {
    "percentage": "Error: Invalid API Key."
  }
}

 How It Works
Spam Detection: The backend uses a scikit-learn pipeline loaded from spam_classifier.pkl. This model was likely trained on a dataset like the SMS Spam Collection using a TF-IDF vectorizer and a classifier (e.g., Naive Bayes). The input text is cleaned and transformed before being fed to the model for prediction.
AI Content Detection: When a request is made, the backend sends the raw text to the Groq API. A carefully crafted system prompt instructs the Llama 3 model to act as an AI detector and return only a number from 0-100, representing the probability of the text being AI-generated.
